{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB\n",
      "(299, 299, 3)\n",
      "(1, 299, 299, 3)\n",
      "[[ 0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"./image22.png\").convert('RGB')\n",
    "print(image.mode)\n",
    "\n",
    "image = np.asarray(image)\n",
    "print(image.shape)\n",
    "image_y = np.zeros(4)\n",
    "image_y[2] = 1\n",
    "\n",
    "image = np.reshape(image, [-1, 299, 299, 3])\n",
    "image_y = np.reshape(image_y, [-1, 4])\n",
    "\n",
    "print (image.shape)\n",
    "print (image_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tsairain_20171031 沒有dataset\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, s, padding):\n",
    "    return tf.nn.conv2d(input=x, filter=W, strides=[1, s, s, 1], padding=padding)\n",
    "\n",
    "def max_pool(x, k, s, padding):\n",
    "    return tf.nn.max_pool(x, ksize= [1, k, k, 1], strides=[1, s, s, 1], padding=padding)\n",
    "\n",
    "def avg_pool(x, k, s, padding):\n",
    "    return tf.nn.avg_pool(x, ksize= [1, k, k, 1], strides=[1, s, s, 1], padding=padding)\n",
    "\n",
    "def Stem(img): #input 299x299x3\n",
    "    w1 = weight_variable([3,3,3,32])\n",
    "    b1 = bias_variable([32])\n",
    "    c1 = conv2d(xs, w1, 2, 'VALID') + b1  #input\n",
    "\n",
    "    w2 = weight_variable([3,3,32,32])\n",
    "    b2 = bias_variable([32])\n",
    "    c2 = conv2d(c1, w2, 1, 'VALID') + b2\n",
    "\n",
    "    w3 = weight_variable([3,3,32,64])\n",
    "    b3 = bias_variable([64])\n",
    "    c3 = conv2d(c2, w3, 1, 'SAME') + b3\n",
    "\n",
    "    #concat1 = conv2 + maxpool\n",
    "    w4 = weight_variable([3,3,64,96])\n",
    "    b4 = bias_variable([96])\n",
    "    c4 = conv2d(c3, w4, 2, 'VALID') + b4\n",
    "    max1 = max_pool(c3, 3, 2, 'VALID')\n",
    "    concat1 = array_ops.concat([c4, max1], 3)\n",
    "\n",
    "    #concat2 = 2次conv2 + 4次conv2\n",
    "\n",
    "    #concat2_left\n",
    "    w5 = weight_variable([1,1,160,64])\n",
    "    b5 = bias_variable([64])\n",
    "    c5 = conv2d(concat1, w5, 1, 'SAME') + b5\n",
    "\n",
    "    w6 = weight_variable([3,3,64,96])\n",
    "    b6 = bias_variable([96])\n",
    "    c6 = conv2d(c5, w6, 1, 'VALID') + b6\n",
    "\n",
    "    #concat2_right\n",
    "    w7 = weight_variable([1,1,160,64])\n",
    "    b7 = bias_variable([64])\n",
    "    c7 = conv2d(concat1, w7, 1, 'SAME') + b7\n",
    "\n",
    "    w8 = weight_variable([7,1,64,64])\n",
    "    b8 = bias_variable([64])\n",
    "    c8 = conv2d(c7, w8, 1, 'SAME') + b8\n",
    "\n",
    "    w9 = weight_variable([1,7,64,64])\n",
    "    b9 = bias_variable([64])\n",
    "    c9 = conv2d(c8, w9, 1, 'SAME') + b9\n",
    "\n",
    "    w10 = weight_variable([3,3,64,96])\n",
    "    b10 = bias_variable([96])\n",
    "    c10 = conv2d(c9, w10, 1, 'VALID') + b10\n",
    "\n",
    "    concat2 = array_ops.concat([c6, c10], 3)\n",
    "\n",
    "    #concat3 = conv2 + maxpool\n",
    "    w11 = weight_variable([3,3,192,192])\n",
    "    b11 = bias_variable([192])\n",
    "    c11 = conv2d(concat2, w11, 2, 'VALID') + b11\n",
    "    max2 = max_pool(concat2, 2, 2, 'VALID')\n",
    "    stem = array_ops.concat([c11, max2], 3)  #output Stem\n",
    "    return stem\n",
    "\n",
    "def InceptionA(img): #input 35x35x384\n",
    "    avg = avg_pool(img, 7, 1, 'SAME')  #ksize不確定\n",
    "    w1 = weight_variable([1,1,384,96])\n",
    "    b1 = bias_variable([96])\n",
    "    c1 = conv2d(avg, w1, 1, 'SAME') + b1\n",
    "    \n",
    "    w2 = weight_variable([1,1,384,96])\n",
    "    b2 = bias_variable([96])\n",
    "    c2 = conv2d(img, w2, 1, 'SAME') + b2\n",
    "    \n",
    "    w3_1 = weight_variable([1,1,384,64])\n",
    "    b3_1 = bias_variable([64])\n",
    "    c3_1 = conv2d(img, w3_1, 1, 'SAME') + b3_1\n",
    "    w3_2 = weight_variable([3,3,64,96])\n",
    "    b3_2 = bias_variable([96])\n",
    "    c3_2 = conv2d(c3_1, w3_2, 1, 'SAME') + b3_2\n",
    "    \n",
    "    w4_1 = weight_variable([1,1,384,64])\n",
    "    b4_1 = bias_variable([64])\n",
    "    c4_1 = conv2d(img, w4_1, 1, 'SAME') + b4_1\n",
    "    w4_2 = weight_variable([3,3,64,96])\n",
    "    b4_2 = bias_variable([96])\n",
    "    c4_2 = conv2d(c4_1, w4_2, 1, 'SAME') + b4_2\n",
    "    w4_3 = weight_variable([3,3,96,96])\n",
    "    b4_3 = bias_variable([96])\n",
    "    c4_3 = conv2d(c4_2, w4_3, 1, 'SAME') + b4_3\n",
    "    \n",
    "    inceptionA = array_ops.concat([c1,c2,c3_2,c4_3], 3)\n",
    "    return inceptionA\n",
    "\n",
    "def ReductionA(img): #input 35x35x384\n",
    "    maxpool = max_pool(img, 3, 2, 'VALID')\n",
    "    \n",
    "    w1 = weight_variable([3,3,384,384])\n",
    "    b1 = bias_variable([384])\n",
    "    c1 = conv2d(img, w1, 2, 'VALID') + b1\n",
    "    \n",
    "    w2_1 = weight_variable([1,1,384,192])\n",
    "    b2_1 = bias_variable([192])\n",
    "    c2_1 = conv2d(img, w2_1, 1, 'SAME') + b2_1\n",
    "    w2_2 = weight_variable([3,3,192,224])\n",
    "    b2_2 = bias_variable([224])\n",
    "    c2_2 = conv2d(c2_1, w2_2, 1, 'SAME') + b2_2\n",
    "    w2_3 = weight_variable([3,3,224,256])\n",
    "    b2_3 = bias_variable([256])\n",
    "    c2_3 = conv2d(c2_2, w2_3, 2, 'VALID') + b2_3\n",
    "    \n",
    "    reductionA = array_ops.concat([maxpool, c1, c2_3], 3)\n",
    "    return reductionA\n",
    "\n",
    "def InceptionB(img): #input 17x17x1024\n",
    "    avg = avg_pool(img, 7, 1, 'SAME')  #ksize不確定\n",
    "    \n",
    "    w1 = weight_variable([1,1,1024,128])\n",
    "    b1 = bias_variable([128])\n",
    "    c1 = conv2d(avg, w1, 1, 'SAME') + b1\n",
    "    \n",
    "    w2 = weight_variable([1,1,1024,384])\n",
    "    b2 = bias_variable([384])\n",
    "    c2 = conv2d(img, w2, 1, 'SAME') + b2\n",
    "    \n",
    "    w3_1 = weight_variable([1,1,1024,192])\n",
    "    b3_1 = bias_variable([192])\n",
    "    c3_1 = conv2d(img, w3_1, 1, 'SAME') + b3_1       \n",
    "    w3_2 = weight_variable([1,7,192,224])\n",
    "    b3_2 = bias_variable([224])\n",
    "    c3_2 = conv2d(c3_1, w3_2, 1, 'SAME') + b3_2       \n",
    "    w3_3 = weight_variable([1,7,224,256])\n",
    "    b3_3 = bias_variable([256])\n",
    "    c3_3 = conv2d(c3_2, w3_3, 1, 'SAME') + b3_3\n",
    "     \n",
    "    w4_1 = weight_variable([1,1,1024,192])\n",
    "    b4_1 = bias_variable([192])\n",
    "    c4_1 = conv2d(img, w4_1, 1, 'SAME') + b4_1\n",
    "    w4_2 = weight_variable([1,7,192,192])\n",
    "    b4_2 = bias_variable([192])\n",
    "    c4_2 = conv2d(c4_1, w4_2, 1, 'SAME') + b4_2\n",
    "    w4_3 = weight_variable([7,1,192,224])\n",
    "    b4_3 = bias_variable([224])\n",
    "    c4_3 = conv2d(c4_2, w4_3, 1, 'SAME') + b4_3\n",
    "    w4_4 = weight_variable([1,7,224,224])\n",
    "    b4_4 = bias_variable([224])\n",
    "    c4_4 = conv2d(c4_3, w4_4, 1, 'SAME') + b4_4\n",
    "    w4_5 = weight_variable([7,1,224,256])\n",
    "    b4_5 = bias_variable([256])\n",
    "    c4_5 = conv2d(c4_4, w4_5, 1, 'SAME') + b4_5\n",
    "    \n",
    "    inceptionB = array_ops.concat([c1, c2, c3_3, c4_5], 3)\n",
    "    return inceptionB\n",
    "\n",
    "def ReductionB(img): #input 17x17x1024\n",
    "    maxpool = max_pool(img, 3, 2, 'VALID')\n",
    "    \n",
    "    w1_1 = weight_variable([1,1,1024,192])\n",
    "    b1_1 = bias_variable([192])\n",
    "    c1_1 = conv2d(img, w1_1, 1, 'SAME') + b1_1\n",
    "    w1_2 = weight_variable([3,3,192,192])\n",
    "    b1_2 = bias_variable([192])\n",
    "    c1_2 = conv2d(c1_1, w1_2, 2, 'VALID') + b1_2\n",
    "    \n",
    "    w2_1 = weight_variable([1,1,1024,256])\n",
    "    b2_1 = bias_variable([256])\n",
    "    c2_1 = conv2d(img, w2_1, 1, 'SAME') + b2_1\n",
    "    w2_2 = weight_variable([1,7,256,256])\n",
    "    b2_2 = bias_variable([256])\n",
    "    c2_2 = conv2d(c2_1, w2_2, 1, 'SAME') + b2_2\n",
    "    w2_3 = weight_variable([7,1,256,320])\n",
    "    b2_3 = bias_variable([320])\n",
    "    c2_3 = conv2d(c2_2, w2_3, 1, 'SAME') + b2_3\n",
    "    w2_4 = weight_variable([3,3,320,320])\n",
    "    b2_4 = bias_variable([320])\n",
    "    c2_4 = conv2d(c2_3, w2_4, 2, 'VALID') + b2_4\n",
    "    \n",
    "    reductionB = array_ops.concat([maxpool, c1_2, c2_4], 3)\n",
    "    return reductionB\n",
    "\n",
    "def InceptionC(img): #input 8x8x1536\n",
    "    avgpool = avg_pool(img, 8, 1, 'SAME')\n",
    "    w1 = weight_variable([1,1,1536,256])\n",
    "    b1 = bias_variable([256])\n",
    "    c1 = conv2d(avgpool, w1, 1, 'SAME') + b1\n",
    "    \n",
    "    w2 = weight_variable([1,1,1536,256])\n",
    "    b2 = bias_variable([256])\n",
    "    c2 = conv2d(img, w2, 1, 'SAME') + b2\n",
    "    \n",
    "    w3 = weight_variable([1,1,1536,384])\n",
    "    b3 = bias_variable([384])\n",
    "    c3 = conv2d(img, w3, 1, 'SAME') + b3\n",
    "    w3_1 = weight_variable([1,3,384,256])\n",
    "    b3_1 = bias_variable([256])\n",
    "    c3_1 = conv2d(c3, w3_1, 1, 'SAME') + b3_1\n",
    "    w3_2 = weight_variable([3,1,384,256])\n",
    "    b3_2 = bias_variable([256])\n",
    "    c3_2 = conv2d(c3, w3_2, 1, 'SAME') + b3_2    \n",
    "    \n",
    "    w4_1 = weight_variable([1,1,1536,384])\n",
    "    b4_1 = bias_variable([384])\n",
    "    c4_1 = conv2d(img, w4_1, 1, 'SAME') + b4_1\n",
    "    w4_2 = weight_variable([1,3,384,448])\n",
    "    b4_2 = bias_variable([448])\n",
    "    c4_2 = conv2d(c4_1, w4_2, 1, 'SAME') + b4_2\n",
    "    w4_3 = weight_variable([3,1,448,512])\n",
    "    b4_3 = bias_variable([512])\n",
    "    c4_3 = conv2d(c4_2, w4_3, 1, 'SAME') + b4_3\n",
    "    w4_31 = weight_variable([3,1,512,256])\n",
    "    b4_31 = bias_variable([256])\n",
    "    c4_31 = conv2d(c4_3, w4_31, 1, 'SAME') + b4_31\n",
    "    w4_32 = weight_variable([1,3,512,256])\n",
    "    b4_32 = bias_variable([256])\n",
    "    c4_32 = conv2d(c4_3, w4_32, 1, 'SAME') + b4_32\n",
    "    \n",
    "    inceptionC = array_ops.concat([c1, c2, c3_1, c3_2, c4_31, c4_32], 3)\n",
    "    return inceptionC\n",
    "    #---------------------------------------------------------------------------    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [None, 299, 299, 3]) #RGB三個通道\n",
    "ys = tf.placeholder(tf.float32, [None, 4])\n",
    "\n",
    "\n",
    "stem = Stem(xs)\n",
    "inceptionA4 = InceptionA(InceptionA(InceptionA(InceptionA(stem))))\n",
    "reductionA = ReductionA(inceptionA4)\n",
    "inceptionB7 = InceptionB(InceptionB(InceptionB(InceptionB(InceptionB(InceptionB(InceptionB(reductionA)))))))\n",
    "reductionB = ReductionB(inceptionB7)\n",
    "inceptionC3 = InceptionC(InceptionC(InceptionC(reductionB)))\n",
    "avgPool = avg_pool(inceptionC3, 8, 1, 'VALID')[:,0,0,:]\n",
    "dropOut = tf.nn.dropout(avgPool, keep_prob=0.8)\n",
    "#softmax\n",
    "W_fc = weight_variable([1536, 4])\n",
    "b_fc = bias_variable([4])\n",
    "output = tf.nn.softmax(tf.matmul(dropOut, W_fc) + b_fc)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(tf.square(ys-output), reduction_indices=[1]))\n",
    "lr = 0.01\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(ys,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "# print('c1.shape = ' + str(c1.shape))\n",
    "# print('c2.shape = ' + str(c2.shape))\n",
    "# print('c3.shape = ' + str(c3.shape))\n",
    "# print('concat1.shape = ' + str(concat1.shape))\n",
    "# print('concat2.shape = ' + str(concat2.shape))\n",
    "# print('stem.shape = ' + str(stem.shape))\n",
    "\n",
    "# print('inceptionA4.shape = ' + str(inceptionA4.shape))\n",
    "# print('reductionA.shape = ' + str(reductionA.shape))\n",
    "# print('inceptionB7.shape = ' + str(inceptionB7.shape))\n",
    "# print('reductionB.shape = ' + str(reductionB.shape))\n",
    "# print('inceptionC3.shape = ' + str(inceptionC3.shape))\n",
    "# print('Average Pooling.shape = ' + str(avgPool.shape))\n",
    "# print('Output.shape = ' + str(output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#計算準確度\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    \n",
    "    print(\"After\")\n",
    "    poo=tf.shape(v_xs)\n",
    "    poosee=tf.Session()\n",
    "    print(poosee.run(poo))\n",
    "    print(\"After\")\n",
    "    print(\"After\")\n",
    "    poo=tf.shape(y_pre)\n",
    "    poosee=tf.Session()\n",
    "    print(poosee.run(poo))\n",
    "    print(\"After\")\n",
    "    print(\"After\")\n",
    "    poo=tf.shape(v_ys)\n",
    "    poosee=tf.Session()\n",
    "    print(poosee.run(poo))\n",
    "    print(\"After\")\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def print_filter(layer, filter_size, num):\n",
    "#     i = 0\n",
    "#     filter_no = 0\n",
    "#     _print = ''\n",
    "    \n",
    "#     for i in range(filter_size):\n",
    "#         _print = ''\n",
    "#         for j in range(filter_size):\n",
    "#             _print = _print + str(sess.run(layer[i,j,num,0])) + '\\t'\n",
    "#         print (_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training accuracy: 0.000000000000, loss: 0.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 100, training accuracy: 0.000000000000, loss: 2.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 200, training accuracy: 0.000000000000, loss: 0.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 300, training accuracy: 0.000000000000, loss: 2.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 400, training accuracy: 0.000000000000, loss: 2.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 500, training accuracy: 1.000000000000, loss: 0.000000\n",
      "[[ 1.  0.  0.  0.]]\n",
      "epoch: 600, training accuracy: 1.000000000000, loss: 2.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 700, training accuracy: 0.000000000000, loss: 0.000000\n",
      "[[ 0.  0.  1.  0.]]\n",
      "epoch: 800, training accuracy: 0.000000000000, loss: 2.000000\n",
      "[[ 1.  0.  0.  0.]]\n",
      "epoch: 900, training accuracy: 1.000000000000, loss: 0.000000\n",
      "[[ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train_step, feed_dict={xs:image, ys:image_y})\n",
    "#    if i == 1:\n",
    "#        print_filter(W_conv1, 7, 2)\n",
    "    if i % 8 == 0:\n",
    "        lr = lr * 0.96\n",
    "    if i % 100 ==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={xs:image, ys:image_y})\n",
    "        train_loss = sess.run(cross_entropy, feed_dict={xs:image, ys:image_y})\n",
    "        print(\"epoch: {}, training accuracy: {:.12f}, loss: {:.6f}\".format(i, train_accuracy, train_loss))\n",
    "        print (sess.run(output, feed_dict={xs:image, ys:image_y}))\n",
    "    \n",
    "\n",
    "# for i in range (1000):\n",
    "#     sess.run(train_step, feed_dict={xs:image, ys:out})  #目前沒有dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
